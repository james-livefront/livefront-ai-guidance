# Part VII: Measurement

## How to Use This Guide

You can’t improve what you don’t measure—but measuring outcomes is tricky. Use these metrics to create shared visibility, learn together, and drive specific actions that improve team performance and client outcomes. Treat measurement as a team activity, not an individual scoreboard.

- Purpose: Align on a few meaningful metrics, not everything you could track.
- Start small: Establish a baseline and pick 1–2 measures per section.
- Team sport: Review with your team and discuss patterns.
- Outcomes over outputs: Favor cycle time, defects, and experience over vanity metrics.
- Action-oriented: Each review should produce 1–2 experiments or improvements.
- Triangulate: Balance speed, quality, and experience to avoid local optimizations.
- No blame: Metrics are for learning and improving the system.

## Speed and Throughput

- Task timing: Track how long common tasks (e.g., writing a component, generating tests, setting up an API client) take with and without AI assistance.
- Cycle time: Measure time from ticket start to PR merged. Look for reductions in repetitive work without slipping in review quality.
- Throughput: Count how many tickets/issues are closed per sprint. Is the team delivering more while holding quality steady?

## Quality and Defects

- Bug types: Categorize bugs found in PR review and QA. Are typos/syntax bugs decreasing while logic/context bugs increasing?
- Escaped defects: Track issues that make it into staging/production. The goal is fewer trivial bugs but no increase in costly edge-case errors.
- Review load: Are PRs easier to review, or are reviewers spending more time untangling AI-generated complexity?

## Developer Experience

- Developer survey: Simple pulse checks ("AI saved me time this week", "AI created more confusion than clarity").
- Cognitive load: Are engineers freed to focus on design, architecture, and problem-solving, or bogged down reviewing AI-generated noise?
- Onboarding speed: Newer engineers should ramp faster if boilerplate and examples are AI-accelerated.

## Client Outcomes

- Delivery predictability: Are we hitting deadlines more consistently?
- Client satisfaction: Is quality meeting expectations? Fewer client-reported bugs?
- Perceived value: Are clients noticing stronger design/UX judgment because engineers have more bandwidth to focus there?

## Balanced Metrics (Avoid Vanity)

Don’t just track "lines of code generated" or "PRs per sprint." Those can be misleading. Instead, balance speed, quality, and experience so that AI helps without eroding Livefront’s standards.
